{
    "docs": [
        {
            "location": "/",
            "text": "taco-read-filtering\n\u00b6\n\n\nThis repository implements a read filtering workflow for\n\ndahak-taco\n.\n\n\nIn this documentation we will cover:\n\n\n\n\n\n\nDownload raw sequence data from OSF data store (wget).\n\n\n\n\n\n\nAssess the quality of the pre-filtered reads (fastqc)\n\n\n\n\n\n\nTrim and filter the reads based on a quality threshold (trimmomatic)\n\n\n\n\n\n\nAssess the quality of the post-filtered reads (fastqc)\n\n\n\n\n\n\nInterleave post-filtered reads (khmer) \n\n\n\n\n\n\nWe will not cover:\n\n\n\n\nHow to install \ntaco\n\n\nThe basics of \ntaco\n \n\n\n\n\nFor those, see the \ndahak-taco documentation\n.\n\n\nSetting Up Taco\n\u00b6\n\n\nTo run these workflows, you must have \ntaco\n installed.\nSee \ndahak-taco documentation\n\nfor installation instructions.\n\n\nOnce \ntaco\n is installed, it will be available on the \ncommand line. The \ntaco\n commands covered in this document\nshould be run from the top-level directory of this repository\n(this is true of any taco workflow repository).\n\n\nWhat's In This Repository\n\u00b6\n\n\nWorkflow files:\n\n\n\n\nrules/\n - contains rule and workflow definitions\n\n\nworkflow-config/\n - contains workflow configuration files\n\n\nworkflow-params/\n - contains workflow parameter files\n\n\ndocker/\n - contains custom Docker images for use in taco workflows\n\n\n\n\nDocumentation files:\n\n\n\n\ndocs/\n - contains the documentation files (i.e., the document you're reading now)\n\n\nmkdocs.yml\n - configuration file for mkdocs, the documentation generator used by this repo\n\n\nmkdocs-material-dib/\n - git submodule containing mkdocs theme\n\n\n\n\nWorkflow\n\u00b6\n\n\nRead Filtering Workflow: Quickstart\n\n\nRead Filtering Workflow: The Rules\n\n\nRead Filtering Workflow: The Parameters\n\n\nRead Filtering Workflow: The Configuration",
            "title": "Index"
        },
        {
            "location": "/#taco-read-filtering",
            "text": "This repository implements a read filtering workflow for dahak-taco .  In this documentation we will cover:    Download raw sequence data from OSF data store (wget).    Assess the quality of the pre-filtered reads (fastqc)    Trim and filter the reads based on a quality threshold (trimmomatic)    Assess the quality of the post-filtered reads (fastqc)    Interleave post-filtered reads (khmer)     We will not cover:   How to install  taco  The basics of  taco     For those, see the  dahak-taco documentation .",
            "title": "taco-read-filtering"
        },
        {
            "location": "/#setting-up-taco",
            "text": "To run these workflows, you must have  taco  installed.\nSee  dahak-taco documentation \nfor installation instructions.  Once  taco  is installed, it will be available on the \ncommand line. The  taco  commands covered in this document\nshould be run from the top-level directory of this repository\n(this is true of any taco workflow repository).",
            "title": "Setting Up Taco"
        },
        {
            "location": "/#whats-in-this-repository",
            "text": "Workflow files:   rules/  - contains rule and workflow definitions  workflow-config/  - contains workflow configuration files  workflow-params/  - contains workflow parameter files  docker/  - contains custom Docker images for use in taco workflows   Documentation files:   docs/  - contains the documentation files (i.e., the document you're reading now)  mkdocs.yml  - configuration file for mkdocs, the documentation generator used by this repo  mkdocs-material-dib/  - git submodule containing mkdocs theme",
            "title": "What's In This Repository"
        },
        {
            "location": "/#workflow",
            "text": "Read Filtering Workflow: Quickstart  Read Filtering Workflow: The Rules  Read Filtering Workflow: The Parameters  Read Filtering Workflow: The Configuration",
            "title": "Workflow"
        },
        {
            "location": "/ReadFiltering/",
            "text": "Read Filtering Workflow\n\u00b6\n\n\nThe \nread_filtering\n workflow uses \n\nfastq\n and \ntrimmomatic\n to assess\nthe quality of sequencer reads and \nfilter out low-quality reads.\n\n\nThe user has control over the names \nand URLs of sequence data that is \ndownloaded, as well as parameters like\nthe quality threshold.\n\n\nQuick Start\n\u00b6\n\n\nTo list available workflows (only one):\n\n\n$ taco ls\n\n\n\n\n\nTo get rules defined by the workflow:\n\n\n$ taco ls read_filtering\n\n\n\n\n\nTo run the workflow using the provided\nworkflow config and parameter files,\n\n\n$ taco read_filtering \\\n    --config-yaml=workflow-config/config_step1.yaml \\\n    --params-yaml=workflow-params/config_step1.yaml\n\n\n\n\n\nBy default, this will generate output files in the \ndata/\n directory.\nTo change the output directory, use the \n--prefix\n flag:\n\n\n$ taco --prefix=my_data read_filtering \\\n    --config-yaml=workflow-config/config_step1.yaml \\\n    --params-yaml=workflow-params/config_step1.yaml\n\n\n\n\n\nIf the directory does not exist, it will be created.\n\n\nContinue reading for more details about how to determine\nwhat rules a workflow defines, how to set parameters\nfor the read filtering workflow, and the various \ntarget files that are useful for the workflow config\nfile.",
            "title": "Read Filtering"
        },
        {
            "location": "/ReadFiltering/#read-filtering-workflow",
            "text": "The  read_filtering  workflow uses  fastq  and  trimmomatic  to assess\nthe quality of sequencer reads and \nfilter out low-quality reads.  The user has control over the names \nand URLs of sequence data that is \ndownloaded, as well as parameters like\nthe quality threshold.",
            "title": "Read Filtering Workflow"
        },
        {
            "location": "/ReadFiltering/#quick-start",
            "text": "To list available workflows (only one):  $ taco ls  To get rules defined by the workflow:  $ taco ls read_filtering  To run the workflow using the provided\nworkflow config and parameter files,  $ taco read_filtering \\\n    --config-yaml=workflow-config/config_step1.yaml \\\n    --params-yaml=workflow-params/config_step1.yaml  By default, this will generate output files in the  data/  directory.\nTo change the output directory, use the  --prefix  flag:  $ taco --prefix=my_data read_filtering \\\n    --config-yaml=workflow-config/config_step1.yaml \\\n    --params-yaml=workflow-params/config_step1.yaml  If the directory does not exist, it will be created.  Continue reading for more details about how to determine\nwhat rules a workflow defines, how to set parameters\nfor the read filtering workflow, and the various \ntarget files that are useful for the workflow config\nfile.",
            "title": "Quick Start"
        },
        {
            "location": "/ReadFilteringRules/",
            "text": "Workflow Rules\n\u00b6\n\n\nThe taco workflow defines a number of rules. For a brief description\nof each, use the \ntaco ls\n command and pass the name of the \n\nread_filtering\n workflow:\n\n\n$ taco ls read_filtering\n\n\n\n\n\nFetch Data Rules:\n\n\npull_biocontainers\n\n    Pull the required versions of containers from quay.io.\n\ndownload_reads\n\n    Fetch user-requested files from OSF\n    containing reads that will be used\n    in the read filtering process.\n\n    Note that this should define wildcard-based\n    rules, rather than downloading all files\n    at once, to keep things flexible and fast.\n\ndownload_read_adapter\n\n    Download FASTA read adapaters.\n    This downloads adpaters for\n    the TruSeq2-PE sequencer by default.\n\n\n\n\n\nQuality Assessment Rules:\n\n\npre_trimming_quality_assessment\n\n    Perform a pre-trimming quality check\n    of the reads from the sequencer.\n\npost_trimming_quality_assessment\n\n    Perform a post-trimming quality check\n    of the reads from the sequencer.\n\n\n\n\n\nFilter/Trim:\n\n\nquality_trimming\n\n    Trim reads from the sequencer by dropping low-quality reads.\n\n\n\n\n\nInterleave:\n\n\ninterleave_reads\n\n    Interleave paired-end reads using khmer.\n    The trim quality comes from the filename.",
            "title": "Read Filtering: Rules"
        },
        {
            "location": "/ReadFilteringRules/#workflow-rules",
            "text": "The taco workflow defines a number of rules. For a brief description\nof each, use the  taco ls  command and pass the name of the  read_filtering  workflow:  $ taco ls read_filtering  Fetch Data Rules:  pull_biocontainers\n\n    Pull the required versions of containers from quay.io.\n\ndownload_reads\n\n    Fetch user-requested files from OSF\n    containing reads that will be used\n    in the read filtering process.\n\n    Note that this should define wildcard-based\n    rules, rather than downloading all files\n    at once, to keep things flexible and fast.\n\ndownload_read_adapter\n\n    Download FASTA read adapaters.\n    This downloads adpaters for\n    the TruSeq2-PE sequencer by default.  Quality Assessment Rules:  pre_trimming_quality_assessment\n\n    Perform a pre-trimming quality check\n    of the reads from the sequencer.\n\npost_trimming_quality_assessment\n\n    Perform a post-trimming quality check\n    of the reads from the sequencer.  Filter/Trim:  quality_trimming\n\n    Trim reads from the sequencer by dropping low-quality reads.  Interleave:  interleave_reads\n\n    Interleave paired-end reads using khmer.\n    The trim quality comes from the filename.",
            "title": "Workflow Rules"
        },
        {
            "location": "/ReadFilteringConfig/",
            "text": "Read Filtering Workflow Configuration\n\u00b6\n\n\nThis page will explain the possible targets that a user\ncan specify when running the \nread_filtering\n workflow.\n\n\nThis would also be a good place for a higher-level\noverview of what this workflow is trying to accomplish,\nbut that could potentially take up a lot of space too.",
            "title": "Read Filtering: Configuration"
        },
        {
            "location": "/ReadFilteringConfig/#read-filtering-workflow-configuration",
            "text": "This page will explain the possible targets that a user\ncan specify when running the  read_filtering  workflow.  This would also be a good place for a higher-level\noverview of what this workflow is trying to accomplish,\nbut that could potentially take up a lot of space too.",
            "title": "Read Filtering Workflow Configuration"
        },
        {
            "location": "/ReadFilteringParams/",
            "text": "Read Filtering Workflow Parameters\n\u00b6\n\n\nThe default parameter dictionary is defined \nin \nread_filtering.settings\n. We include a few\nnotes on each portion of this file.\n\n\nfrom snakemake.utils import update_config\n\nif(not config['clean']):\n    ...\n\n\n\n\n\nNote that if the \n--clean\n flag is specified, \nthis default configuration is not set.\nThis can be useful for troubleshooting \nor ensuring all input parameters have been\nspecified.\n\n\n    # Note: don't include http:// or https://\n    config_default = {\n\n\n\n\n\nThe structure of the default config dictionary\nis covered on the \nWorkflows\n\npage of the taco documentation.\nIt consists of top-level keys named\nfor the workflow, or for the general \n\"common\" keys used by most or all rules\n(e.g., biocontainers). \n\n\nWe described the parameter dictionary structure as:\n\n\n{\n    '<workflow-name>' : {\n        '<rule-name>' : {\n            '<param-name>' : <param-value>,\n            '<param-list>' : [<value1>, ...],\n            ...\n        }\n    }\n}\n\n\n\n\n\nWe see this structure below with two top-level\nkeys, \"biocontainers\" and \"read_filtering\".\n\n\n        \"biocontainers\" : {\n            \"trimmomatic\" : {\n                \"use_local\" : False,\n                \"quayurl\" : \"quay.io/biocontainers/trimmomatic\",\n                \"version\" : \"0.36--5\"\n            },\n            \"fastqc\" : {\n                \"use_local\" : False,\n                \"quayurl\" : \"quay.io/biocontainers/fastqc\",\n                \"version\" : \"0.11.7--pl5.22.0_2\"\n            },\n            \"khmer\" : {\n                \"use_local\" : False,\n                \"quayurl\" : \"quay.io/biocontainers/khmer\",\n                \"version\" : \"2.1.2--py35_0\"\n            }\n        },\n\n\n\n\n\nThe biocontainers key stores all information about \ncontainer images for different programs used in this \nworkflow. This information is shared across all rules\nso it is grouped under a common top-level key.\n\n\nThe other top-level key is the key named for the workflow.\n\n\n        \"read_filtering\" : {\n\n            # Note: read files (below) must match pre-trimming-pattern below.\n            # The workflow actually builds the rules to download \n            # the read files by using the pre_trimming_pattern.\n            \"read_patterns\" : {\n                \"pre_trimming_pattern\"  : \"{sample}_{direction}_reads.fq.gz\",\n                \"post_trimming_pattern\" : \"{sample}_{direction}_trim{qual}.fq.gz\",\n            },\n\n            # read_files must be defined by user \n            \"read_files\" : {\n                \"SRR606249_1_reads.fq.gz\" :           \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f0f9156c613b026430dbc7\",\n                \"SRR606249_2_reads.fq.gz\" :           \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f0fc7fb83f69026076be47\",\n                \"SRR606249_subset10_1_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f10134b83f69026377611b\",\n                \"SRR606249_subset10_2_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f101f26c613b026330e53a\",\n                \"SRR606249_subset25_1_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f1039a594d900263120c38\",\n                \"SRR606249_subset25_2_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f104ed594d90026411f486\",\n                \"SRR606249_subset50_1_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f1082d6c613b026430e5cf\",\n                \"SRR606249_subset50_2_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f10ac6594d900262123e77\"\n            },\n\n            \"quality_assessment\" : {\n                # optional, modifiers for the .fq.gz --> .zip --> results workflow\n                \"fastqc_suffix\": \"fastqc\",\n            },\n\n            \"quality_trimming\" : {\n                \"trim_suffix\" : \"se\"\n            },\n            \"interleaving\" : {\n                \"interleave_suffix\" : \"pe\"\n            },\n            \"adapter_file\" : {\n                \"name\" : \"TruSeq2-PE.fa\",\n                \"url\"  : \"http://dib-training.ucdavis.edu.s3.amazonaws.com/mRNAseq-semi-2015-03-04/TruSeq2-PE.fa\"\n            }\n        }\n    }\n\n\n\n\n\nThere are keys corresponding to each rule or application.\nThis information is used to control the behavior of the \nrules. For some simple examples of how to use these\nparameters to construct rules, see the \ntaco-simple\n\nworkflow repository.",
            "title": "Read Filtering: Parameters"
        },
        {
            "location": "/ReadFilteringParams/#read-filtering-workflow-parameters",
            "text": "The default parameter dictionary is defined \nin  read_filtering.settings . We include a few\nnotes on each portion of this file.  from snakemake.utils import update_config\n\nif(not config['clean']):\n    ...  Note that if the  --clean  flag is specified, \nthis default configuration is not set.\nThis can be useful for troubleshooting \nor ensuring all input parameters have been\nspecified.      # Note: don't include http:// or https://\n    config_default = {  The structure of the default config dictionary\nis covered on the  Workflows \npage of the taco documentation.\nIt consists of top-level keys named\nfor the workflow, or for the general \n\"common\" keys used by most or all rules\n(e.g., biocontainers).   We described the parameter dictionary structure as:  {\n    '<workflow-name>' : {\n        '<rule-name>' : {\n            '<param-name>' : <param-value>,\n            '<param-list>' : [<value1>, ...],\n            ...\n        }\n    }\n}  We see this structure below with two top-level\nkeys, \"biocontainers\" and \"read_filtering\".          \"biocontainers\" : {\n            \"trimmomatic\" : {\n                \"use_local\" : False,\n                \"quayurl\" : \"quay.io/biocontainers/trimmomatic\",\n                \"version\" : \"0.36--5\"\n            },\n            \"fastqc\" : {\n                \"use_local\" : False,\n                \"quayurl\" : \"quay.io/biocontainers/fastqc\",\n                \"version\" : \"0.11.7--pl5.22.0_2\"\n            },\n            \"khmer\" : {\n                \"use_local\" : False,\n                \"quayurl\" : \"quay.io/biocontainers/khmer\",\n                \"version\" : \"2.1.2--py35_0\"\n            }\n        },  The biocontainers key stores all information about \ncontainer images for different programs used in this \nworkflow. This information is shared across all rules\nso it is grouped under a common top-level key.  The other top-level key is the key named for the workflow.          \"read_filtering\" : {\n\n            # Note: read files (below) must match pre-trimming-pattern below.\n            # The workflow actually builds the rules to download \n            # the read files by using the pre_trimming_pattern.\n            \"read_patterns\" : {\n                \"pre_trimming_pattern\"  : \"{sample}_{direction}_reads.fq.gz\",\n                \"post_trimming_pattern\" : \"{sample}_{direction}_trim{qual}.fq.gz\",\n            },\n\n            # read_files must be defined by user \n            \"read_files\" : {\n                \"SRR606249_1_reads.fq.gz\" :           \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f0f9156c613b026430dbc7\",\n                \"SRR606249_2_reads.fq.gz\" :           \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f0fc7fb83f69026076be47\",\n                \"SRR606249_subset10_1_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f10134b83f69026377611b\",\n                \"SRR606249_subset10_2_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f101f26c613b026330e53a\",\n                \"SRR606249_subset25_1_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f1039a594d900263120c38\",\n                \"SRR606249_subset25_2_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f104ed594d90026411f486\",\n                \"SRR606249_subset50_1_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f1082d6c613b026430e5cf\",\n                \"SRR606249_subset50_2_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f10ac6594d900262123e77\"\n            },\n\n            \"quality_assessment\" : {\n                # optional, modifiers for the .fq.gz --> .zip --> results workflow\n                \"fastqc_suffix\": \"fastqc\",\n            },\n\n            \"quality_trimming\" : {\n                \"trim_suffix\" : \"se\"\n            },\n            \"interleaving\" : {\n                \"interleave_suffix\" : \"pe\"\n            },\n            \"adapter_file\" : {\n                \"name\" : \"TruSeq2-PE.fa\",\n                \"url\"  : \"http://dib-training.ucdavis.edu.s3.amazonaws.com/mRNAseq-semi-2015-03-04/TruSeq2-PE.fa\"\n            }\n        }\n    }  There are keys corresponding to each rule or application.\nThis information is used to control the behavior of the \nrules. For some simple examples of how to use these\nparameters to construct rules, see the  taco-simple \nworkflow repository.",
            "title": "Read Filtering Workflow Parameters"
        }
    ]
}